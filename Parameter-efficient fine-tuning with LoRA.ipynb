{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_unzip_spam_data(\n",
    "        url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download \"\n",
    "              \"and extraction.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    #1 Downloads the file\n",
    "    with urllib.request.urlopen(url) as response:    \n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    #2 Unzips the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:    \n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "\n",
    "    #3 Adds a .tsv file extension\n",
    "    os.rename(original_file_path, data_file_path)               \n",
    "    print(f\"File downloaded and saved as {data_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    # 1 Counts the instances of “spam”\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # 2 Randomly samples \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
    "        num_spam, random_state = 123\n",
    "    )\n",
    "\n",
    "    # 3 Combine ham subset with \"spam\"\n",
    "    balanced_df = pd.concat([\n",
    "        ham_subset, df[df[\"Label\"] == \"spam\"]\n",
    "    ])\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    #1 Shuffles the entire DataFrame\n",
    "    df = df.sample(\n",
    "        frac = 1, random_state = 123\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    #2 Calculates split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    #3 Splits the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file_path, sep=\"\\t\",\n",
    "                 header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating PyTorch datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length = None,\n",
    "                 pad_token_id = 50256):        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        #1 Pretokenizes texts\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "        \n",
    "        #2 Truncates sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        #3 Pads sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype = torch.long),\n",
    "            torch.tensor(label, dtype = torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        \n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None,\n",
    "                            tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\n",
    "    \"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    \"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating PyTorch data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: \n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader: \")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "#1 Vocabulary size\n",
    "#2 Context length\n",
    "#3 Dropout rate\n",
    "#4 Query-key-value bias\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,         #1\n",
    "    \"context_length\": 1024,      #2\n",
    "    \"drop_rate\": 0.0,            #3\n",
    "    \"qkv_bias\": True             #4\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace output layer to num_class = 2\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches = None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Ensures number of batches doesn’t exceed batches in data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # .to(device): use GPU or CPU\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Logits of last output token\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "\n",
    "                # find the predicted labels with max softmax probabilities \n",
    "                #   that only has two values in a vector\n",
    "                predicted_labels = torch.argmax(logits, dim = -1)\n",
    "            \n",
    "            # find the number of labels\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "\n",
    "            # find the correct prediction\n",
    "            correct_predictions += (\n",
    "                (predicted_labels == target_batch).sum().item()\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # return average accuracy\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter-efficient fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        # 1\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a = math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        # original linear layer\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        # 1 Replaces the Linear layer with LinearWithLoRA\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # 2  Recursively applies the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "# freeze the original model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from previous_chapters import evaluate_model, calc_loss_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "        model, train_loader, val_loader, \n",
    "        optimizer, device, num_epochs, eval_freq, eval_iter):\n",
    "    \n",
    "    # 1 Initialize lists to track losses and examples seen\n",
    "\n",
    "    # training and validation loss\n",
    "    # training exmaples seen, validation examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    # 2 Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # 3 Sets model to training mode\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 4 Resets loss gradients from the previous batch iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "\n",
    "            # 5 Calculates loss gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 6 Updates model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # 7 New: tracks examples instead of tokens\n",
    "            examples_seen += input_batch.shape[0]\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # 8 Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        \n",
    "        # 9 Calculates accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.008, Val loss 0.051\n",
      "Ep 2 (Step 000250): Train loss 0.022, Val loss 0.179\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.076, Val loss 0.064\n",
      "Ep 3 (Step 000350): Train loss 0.009, Val loss 0.149\n",
      "Training accuracy: 100.00% | Validation accuracy: 92.50%\n",
      "Ep 4 (Step 000400): Train loss 0.043, Val loss 0.090\n",
      "Ep 4 (Step 000450): Train loss 0.144, Val loss 0.333\n",
      "Ep 4 (Step 000500): Train loss 0.045, Val loss 0.134\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.004, Val loss 0.219\n",
      "Ep 5 (Step 000600): Train loss 0.004, Val loss 0.302\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Training completed in 2.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(\n",
    "    epochs_tensor, examples_seen_tensor, \n",
    "    train_losses, val_losses, label=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.42%\n",
      "Validation accuracy: 95.97%\n",
      "Test accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
