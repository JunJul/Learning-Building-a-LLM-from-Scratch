{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Creates a zero-dimensional tensor (scalar) from a Python integer\n",
    "tensor0d = torch.tensor(1)\n",
    "\n",
    "# 2 Creates a one-dimensional tensor (vector) from a Python list\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 3 Creates a two-dimensional tensor from a nested Python list\n",
    "tens0r2d = torch.tensor([[1, 2],\n",
    "                        [3, 4]])\n",
    "\n",
    "# 4 Creates a three-dimensional tensor from a nested Python list\n",
    "tensor3d = torch.tensor([\n",
    "    [[1, 2], [3, 4]],\n",
    "    [[5, 6], [7, 8]]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = tensor1d.to(torch.float32)\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common PyTorch tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "tensor2d = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "print(tensor2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.reshape(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.view(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.matmul(tensor2d.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d @ tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A logistic regression forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 This import statement is a common convention in PyTorch to prevent long lines of code.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 2 True label\n",
    "y = torch.tensor([1.0])\n",
    "\n",
    "# 3 Input feature\n",
    "x1 = torch.tensor([1.1])\n",
    "\n",
    "# 4 Weight parameter\n",
    "w1 = torch.tensor([2.2])\n",
    "\n",
    "# 5 Bias unit\n",
    "b = torch.tensor([0.0])\n",
    "\n",
    "# 6 Net input\n",
    "z = x1 * w1 + b\n",
    "\n",
    "# 7 Activation and output\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients via autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad = True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "#1 By default, PyTorch destroys the computation graph after calculating \n",
    "#   the gradients to free memory. However, since we will reuse this computation graph shortly, we set retain_graph=True so that it stays in memory.\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "# For instance, we can call .backward on the loss, and \n",
    "#   PyTorch will compute the gradients of all the leaf nodes in the graph, \n",
    "#   which will be stored via the tensors’ .grad attributes:\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multilayer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing a neural network in PyTorch, we can subclass the torch.nn.Module class to define our own custom network architecture. This Module base class provides a lot of functionality, making it easier to build and train models. For instance, it allows us to encapsulate layers and operations and keep track of the model’s parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this subclass, we define the network layers in the __init__ constructor and specify how the layers interact in the forward method. The forward method describes how the input data passes through the network and comes together as a computation graph. In contrast, the backward method, which we typically do not need to implement ourselves, is used during training to compute gradients of the loss function given the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    # 1 Coding the number of inputs and outputs as variables allows us \n",
    "    # to reuse the same code for datasets with different numbers of features and classes\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "\n",
    "            # 1st hidden layer\n",
    "            # 2 The Linear layer takes the number of input and output nodes as arguments.\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            # 3 Nonlinear activation functions are placed between the hidden layers.\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            # 4 The number of output nodes of one hidden layer has to match the number of inputs of the next layer.\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        # 5 The outputs of the last layer are called logits.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_inputs=50, num_outputs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters:  2213\n"
     ]
    }
   ],
   "source": [
    "# The total number of trainable parameters of this model\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our neural network model with the preceding two hidden layers, these trainable parameters are contained in the torch.nn.Linear layers. A Linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes referred to as a feedforward or fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1388,  0.0159,  0.1215,  ...,  0.1032,  0.0296,  0.0102],\n",
      "        [ 0.0229,  0.0260, -0.0458,  ..., -0.0358,  0.0362,  0.0497],\n",
      "        [-0.0896,  0.0113,  0.1370,  ...,  0.1037,  0.1230, -0.0929],\n",
      "        ...,\n",
      "        [-0.1362, -0.0713, -0.0010,  ...,  0.1176,  0.1054, -0.1012],\n",
      "        [ 0.1226,  0.0937, -0.1409,  ...,  0.1321, -0.0613,  0.0086],\n",
      "        [-0.0045, -0.0604,  0.0535,  ...,  0.0697,  0.0373,  0.0923]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights are initialized with small random numbers, which differ each time we instantiate the network. In deep learning, initializing model weights with small random numbers is desired to break symmetry during training. Otherwise, the nodes would be performing the same operations and updates during backpropagation, which would not allow the network to learn complex mappings from inputs to outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "X = torch.rand((1, 50))\n",
    "out = model(X)\n",
    "\n",
    "# a score assigned to each of the three output nodes\n",
    "# The <AddmmBackward0> part of grad_fn=<AddmmBackward0> specifies the operation performed. \n",
    "# In this case, it is an Addmm operation. \n",
    "# Addmm stands for matrix multiplication (mm) followed by an addition (Add).\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we use a model for inference (for instance, making predictions) rather than training, the best practice is to use the torch.no_grad() context manager. This tells PyTorch that it doesn’t need to keep track of the gradients, which can result in significant savings in memory and computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "# compute class-membership probabilities\n",
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim = 1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up efficient data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a small toy dataset\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom Dataset class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "    \n",
    "    # 1 Instructions for retrieving exactly one data record and the corresponding label\n",
    "    def __getitem__(self, index):\n",
    "       one_x = self.features[index]\n",
    "       one_y = self.labels[index]\n",
    "       return one_x, one_y\n",
    "\n",
    "    # 2 Instructions for returning the total length of the dataset\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    # 1 The ToyDataset instance created earlier serves as input to the data loader.\n",
    "    dataset = train_ds,\n",
    "    batch_size = 2,\n",
    "    # 2 Whether or not to shuffle the data\n",
    "    shuffle = True,\n",
    "    # 3 The number of background processes\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    #4 It is not necessary to shuffle a test dataset.\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:  tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 2:  tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 3:  tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx + 1}: \", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A typical training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01/003 | Batch 000/002 | Train Loss: 0.75\n",
      "Epoch:  01/003 | Batch 001/002 | Train Loss: 0.65\n",
      "Epoch:  02/003 | Batch 000/002 | Train Loss: 0.44\n",
      "Epoch:  02/003 | Batch 001/002 | Train Loss: 0.13\n",
      "Epoch:  03/003 | Batch 000/002 | Train Loss: 0.03\n",
      "Epoch:  03/003 | Batch 001/002 | Train Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Neural Network training in PyTorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 1 The dataset has two features and two classes.\n",
    "model = NeuralNetwork(num_inputs = 2, num_outputs = 2)\n",
    "\n",
    "# 2 The optimizer needs to know which parameters to optimize.\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = 0.5\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # 3 Sets the gradients from the previous round to 0 to prevent unintended gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # 4 Computes the gradients of the loss given the model parameters\n",
    "        loss.backward()\n",
    "        # 5 The optimizer uses the gradients to update the model parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Logging\n",
    "        print(  f\"Epoch: {epoch+1: 03d}/{num_epochs:03d}\"\n",
    "                f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "                f\" | Train Loss: {loss:.2f}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Insert optional model evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001DA48BEE490>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim = 1)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(probas, dim = 1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to compute the prediction accuracy\n",
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim = 1)\n",
    "        \n",
    "        # 1 Returns a tensor of True/False values depending on whether the labels match\n",
    "        compare = labels == predictions\n",
    "\n",
    "        # 2 The sum operation counts the number of True values.\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    # 3 The fraction of correct prediction, a value between 0 and 1.\n",
    "    #  .item() returns the value of the tensor as a Python float.\n",
    "    return (correct / total_examples).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.load(\"model.pth\") function reads the file \"model.pth\" and reconstructs the Python dictionary object containing the model’s parameters while model.load_state_dict() applies these parameters to the model, effectively restoring its learned state from when we saved it.\n",
    "\n",
    "The line model = NeuralNetwork(2, 2) is not strictly necessary if you execute this code in the same session where you saved a model. However, I included it here to illustrate that we need an instance of the model in memory to apply the saved parameters. Here, the NeuralNetwork(2, 2) architecture needs to match the original saved model exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\42128\\AppData\\Local\\Temp\\ipykernel_11044\\44082859.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(2, 2)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing training performance with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# PyTorch computations on GPU devices\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.tensor([1., 2., 3.])\n",
    "tensor_2 = torch.tensor([4., 5., 6.])\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = tensor_1.to(\"cuda\")\n",
    "tensor_2 = tensor_2.to(\"cuda\")\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tensor_1 \u001b[38;5;241m=\u001b[39m tensor_1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtensor_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_2\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "tensor_1 = tensor_1.to(\"cpu\")\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Single-GPU training\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "device = torch.device(\"cuda\")      #1 Defines a device variable that defaults to a GPU\n",
    "model = model.to(device)          #2 Transfers the model onto the GPU\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)   #3 Transfers the data onto the GPU\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels) # Loss function\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    # Insert optional model evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch’s DistributedDataParallel (DDP) strategy. DDP enables parallelism by splitting the input data across the available devices and processing these data subsets simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with multiple GPUs\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch’s multiprocessing submodule contains functions such as multiprocessing .spawn, which we will use to spawn multiple processes and apply a function to multiple inputs in parallel. We will use it to spawn one training process per GPU. If we spawn multiple processes for training, we will need a way to divide the dataset among these different processes. For this, we will use the DistributedSampler.\n",
    "\n",
    "init_process_group and destroy_process_group are used to initialize and quit the distributed training mods. The init_process_group function should be called at the beginning of the training script to initialize a process group for each process in the distributed setup, and destroy_process_group should be called at the end of the training script to destroy a given process group and release its resources. The code in the following listing illustrates how these new components are used to implement DDP training for the NeuralNetwork model we implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training with the DistributedDataParallel strategy\n",
    "def ddp_setup(rank, world_size):\n",
    "    # 1 Address of the main node\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    # 2 Any free port on the machine\n",
    "    os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "\n",
    "    init_process_group(\n",
    "        # 3 nccl stands for NVIDIA Collective Communication Library.\n",
    "        backend=\"nccl\",\n",
    "        # 4 rank refers to the index of the GPU we want to use.\n",
    "        rank=rank,\n",
    "        # 5 world_size is the number of GPUs to use.\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "    # 6 Sets the current GPU device on which tensors \n",
    "    # will be allocated and operations will be performed\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def prepare_dataset():\n",
    "    # insert dataset preparation code\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=2,\n",
    "\n",
    "        # 7 Distibuted-Sampler takes care of the shuffling now.\n",
    "        shuffle=False,\n",
    "\n",
    "        # 8 Enables faster memory transfer when training on GPU\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "\n",
    "        # 9  Splits the dataset into distinct, non-overlapping subsets for each process (GPU)\n",
    "        sampler=DistributedSampler(train_ds)\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 The main function running the model training\n",
    "def main(rank, world_size, num_epochs):\n",
    "    ddp_setup(rank, world_size)\n",
    "    train_loader, test_loader = prepare_dataset()\n",
    "    model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "    model.to(rank)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for features, labels in train_loader:\n",
    "            # 11 rank is the GPU ID\n",
    "            features, labels = features.to(rank), labels.to(rank)\n",
    "            # insert model prediction and backpropagation code\n",
    "            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "                    f\" | Batchsize {labels.shape[0]:03d}\"\n",
    "                    f\" | Train/Val Loss: {loss:.2f}\")\n",
    "                \n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, device=rank)\n",
    "    print(f\"[GPU{rank}] Training accuracy\", train_acc)\n",
    "    test_acc = compute_accuracy(model, test_loader, device=rank)\n",
    "    print(f\"[GPU{rank}] Test accuracy\", test_acc)\n",
    "    # 12 Cleans up resource allocation\n",
    "    destroy_process_group()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      5\u001b[0m world_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\42128\\miniconda3\\lib\\site-packages\\torch\\multiprocessing\\spawn.py:282\u001b[0m, in \u001b[0;36mspawn\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    276\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\42128\\miniconda3\\lib\\site-packages\\torch\\multiprocessing\\spawn.py:238\u001b[0m, in \u001b[0;36mstart_processes\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\42128\\miniconda3\\lib\\site-packages\\torch\\multiprocessing\\spawn.py:178\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[0;32m    172\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m    176\u001b[0m         )\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[0;32m    180\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m    181\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[0;32m    182\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[0;32m    183\u001b[0m         )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_files[error_index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[0;32m    186\u001b[0m     original_trace \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fh)\n",
      "\u001b[1;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "    torch.manual_seed(123)\n",
    "    num_epochs = 3\n",
    "    world_size = torch.cuda.device_count()\n",
    "    \n",
    "    # 13 Launches the main function using multiple processes, \n",
    "    # where nprocs=world_size means one process per GPU.\n",
    "    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
